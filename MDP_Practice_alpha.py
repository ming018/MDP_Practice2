# import numpy as np

# field = np.zeros((22, 22))

# # 은신 요소
# # -3 : 인구 밀집 지역, -2 고속 도로, -1 일반 도로 혹은 비포장 도로, 0 요소 없음 1 : 경작지, 3 : 초지
HIDE =   [[  3, -3,  3,  3,  3,  3,  3,  0,  0,  3, -1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],
          [ -3, -3,  3, -1, -1, -1, -1,  3,  0,  3, -1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],
          [ -1, -3, -1,  3,  3,  3,  3, -1,  3,  3, -1, -1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],
          [ -3, -1,  3,  3,  0,  0,  0,  3, -1,  3, -1,  3, -1,  3,  0,  0,  0,  0,  0,  0,  0,  0],
          [ -3, -3, -1,  3,  0,  0,  0,  0,  3, -1,  3,  0,  3, -1,  3,  3,  3,  3,  0,  0,  0,  0],
          [ -3, -3, -1,  3,  0,  0,  0,  3, -1,  3,  0,  0,  0,  3, -1, -1, -1,  3,  0,  0,  0,  3],
          [ -3, -1, -3,  3,  0,  0,  3, -1,  3,  0,  0,  0,  0,  3, -1,  3, -1,  3,  0,  0,  0,  3],
          [ -1, -3, -3,  3,  0,  3, -1,  3,  0,  0,  0,  0,  0,  3, -1,  3,  3, -1,  3,  0,  3, -2],
          [ -3, -1, -3,  3,  0,  3, -1,  3,  0,  0,  0,  0,  0,  3, -1,  3,  0,  3, -1,  3, -2, -3],
          [ -1, -3, -3,  3,  0,  3, -1,  3,  0,  0,  0,  0,  0,  3, -1,  3,  0,  0,  3, -1, -2, -3],
          [ -3, -1, -3,  3,  3, -1,  0,  0,  0,  0,  0,  0,  0,  3, -1,  3,  0,  0,  0,  3, -2, -3],
          [ -3, -1, -3,  3, -1,  3,  0,  0,  0, -1, -1, -1, -1,  3,  3, -1,  3,  0,  0,  3, -2, -3],
          [ -1, -3,  3,  3, -1,  3,  0,  0, -1,  3,  3,  3,  3, -1, -1, -1, -1,  3,  0,  3,  1, -2],
          [ -1,  3,  3,  3, -1,  3,  0, -1,  3,  0,  0,  0,  0,  3,  3,  3,  3, -1,  3,  3,  1,  1],
          [  3, -1,  3,  3, -1,  3, -1,  3,  3,  0,  0,  0,  0,  0,  0,  0,  3, -1,  3,  3,  1,  1],
          [  3,  3, -1,  3,  3, -1,  3, -1,  3,  0,  0,  0,  0,  0,  0,  0,  3,  3, -1,  3,  1,  1],
          [  0,  3, -1,  3,  3, -1,  3,  3, -1,  0,  0,  0,  0,  0,  0,  0,  0,  3,  3, -2, -2,  1],
          [  0,  3, -1,  3,  3, -1,  3,  0,  3, -1,  0,  0,  0,  0,  0,  0,  0,  0,  3, -2,  3, -2],
          [  3,  3, -1,  3,  3, -1,  3,  0,  3,  3, -1,  0,  0,  0,  0,  0,  0,  0,  3, -2,  3,  3],
          [  3, -1,  3,  3, -1, -1,  3,  0,  0,  0,  3, -1, -1, -1,  0,  0,  0,  0,  3,  3, -2,  3],
          [  3, -1,  3,  3, -1,  3,  3,  0,  0,  0,  0,  3,  3,  3, -1,  0,  0,  0,  0,  3,  3, -2] ]

# 체력 요소
# -3 : 강/하천, 급류 -2 : 절벽 0:아무 요소 없음 1:산길
HP = [ 
        [  0,  0,  0,  0,  0,  0,  0, -2, -2,  0,  0,  0, -3, -3,  1, -3,  1,  1, -3, -3, -3, -3],
        [  0,  0,  0,  0,  0,  0,  0,  0, -2,  0,  0,  0, -3, -3,  1,  1, -3,  1,  1, -3,  1,  1],
        [  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -3,  1,  1,  1, -3, -3,  1,  1, -3],
        [  0,  0,  0,  0,  1,  1,  1,  0,  0,  0,  0,  0,  0,  0,  1, -3,  1,  1,  1, -3, -3,  1],
        [  0,  0,  0,  0,  1,  1,  1,  1,  0,  0,  0,  1,  0,  0,  0,  0,  0,  1,  1,  1,  1,  0],
        [  0,  0,  0,  0,  1,  1,  1,  0,  0,  0,  1,  1,  1,  0,  0,  0,  0,  0, -3,  1, -3,  0],
        [  0,  0,  0,  0,  1,  1,  0,  0,  0,  1,  1,  1,  1,  0,  0,  0,  0,  0, -3, -3, -3,  0],
        [  0,  0,  0,  0,  1,  0,  0,  0,  1, -2, -3,  1,  1,  0,  0,  0,  0,  0,  0, -3,  0,  0],
        [  0,  0,  0,  0,  1,  0,  0,  0,  1, -2, -3, -3,  1,  0,  0,  0, -3,  0,  0,  0,  0,  0],
        [  0,  0,  0,  0,  1,  0,  0,  0,  1, -3,  1,  1,  1,  0,  0,  0, -3, -3,  0,  0,  0,  0],
        [  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  0,  0,  0, -3, -3, -3,  0,  0,  0],
        [  0,  0,  0,  0,  0,  0,  1,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0, -3, -3,  0,  0,  0],
        [  0,  0,  0,  0,  0,  0,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -3,  0,  1,  0],
        [  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1],
        [  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  0,  0,  0,  0,  0,  0,  0,  1,  1],
        [  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  0,  0, -2,  1,  1,  0,  0,  0,  0,  1,  1],
        [  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  1,  1,  0,  0,  0,  0,  0,  1],
        [ -3,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  0,  0,  0,  0,  1,  1,  0,  0,  0,  0,  0],
        [ -3,  0,  0,  0,  0,  0,  0,  1,  0,  0,  1,  1,  0,  0,  0,  1,  1,  1,  0,  0,  0,  0],
        [  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  0,  0,  0,  0],
        [  0,  0,  0,  0,  0,  0,  0, -2, -2,  1,  0,  0,  0,  0,  1,  1,  1,  1,  0,  0,  0,  0],
        [  0,  0,  0,  0,  0,  0,  0,  1, -3,  1,  0,  0,  0,  0,  0,  1,  1,  1,  1,  0,  0,  0],
        [  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  0,  0,  0,  0,  0,  1,  1,  1,  1,  0,  0]
           ]


SLOPE = [  

        [  1, -2,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  0,  0, -1,  0, -2, -2,  0,  0,  0,  0],
        [  1, -2,  1,  0, -1, -1,  0,  1,  1,  1, -1,  1,  0,  0,  1, -1,  0,  1, -2,  0, -2, -2],
        [  0,  1, -1,  1,  1,  1,  1,  0,  1,  1,  0, -1,  1,  1,  1,  0, -1,  0,  0, -1, -2,  0],
        [  1,  0,  1,  0,  0,  0,  0,  1, -1,  1,  0,  1,  0,  1,  1,  1,  1, -1, -1,  0,  0,  0],
        [ -2,  0,  1,  0,  0, -1, -1,  0,  1,  0,  1,  0,  1, -1,  1,  1,  1,  1,  0, -1, -1,  1],
        [ -2,  0,  1,  0,  0, -1, -1,  1, -1,  1,  0,  0,  0,  1, -1,  0, -1,  1,  1,  1,  1,  1],
        [ -2,  0,  0,  0,  0,  0,  1,  0,  1,  0, -1, -1, -1,  1, -1,  1,  0,  1,  1,  1,  1,  1],
        [  1,  0,  0,  0,  0,  1, -1,  1,  0, -2,  0, -1,  0,  1, -1,  1,  1, -1,  1,  1,  1, -1],
        [ -2,  1,  0,  0,  0,  1, -1,  1,  0, -2,  0,  0,  0,  1,  0,  1,  0,  0, -1,  1, -1,  0],
        [  0,  1,  0,  0,  0,  1,  0,  1, -1,  0, -1, -1, -1,  1,  0,  1,  0,  0,  1,  0,  0,  1],
        [ -2,  1,  0,  0,  1,  0,  1,  0,  0, -1,  0,  0,  0,  1, -1,  1,  0,  0,  0,  1, -1,  1],
        [  1,  1,  1,  0, -1,  1, -1, -1, -1,  0, -1, -1,  0,  1,  1, -1,  1,  0,  0,  1, -1,  0],
        [  1,  1,  1,  0,  0,  1,  0,  0, -1,  1,  1,  1,  1,  0,  0,  0,  0,  1,  0,  1,  1, -1],
        [  0,  1,  1,  0, -1,  1,  0, -1,  1,  0, -1,  0,  0,  1,  1,  1,  1,  0,  1,  1,  1,  1],
        [  1,  1,  1,  0,  0,  1, -1,  1,  1, -1, -1, -2, -2, -2, -1, -1,  1,  0,  1,  1,  1,  1],
        [  1,  1, -1,  0,  1,  0,  1, -1,  1,  0, -2,  0,  0,  0,  0,  0,  1,  1,  0,  1,  1,  1],
        [  0,  1, -1,  0,  1, -1,  1,  1,  0,  0,  0, -2,  0,  0,  0,  0,  0,  1,  1, -1, -1,  1],
        [  0,  1,  0,  0,  1, -1,  1,  0,  1, -1,  0,  0, -2, -2,  0, -1, -1, -1,  1, -1,  1, -1],
        [  1,  1,  0,  0,  1,  0,  1,  0,  1,  1, -1,  0,  0,  0,  0, -1, -1, -1,  1, -1,  1,  1],
        [  1, -1,  1,  0, -1,  0,  1, -2, -2,  0,  1,  0, -1, -1,  0, -1, -1, -1,  1,  1, -1,  1],
        [  1, -1,  1,  0, -1,  0,  1, -2, -2,  0,  1,  0, -1, -1,  0, -1, -1, -1,  1,  1, -1,  1],
        [  1, -1,  1,  0, -1,  1,  1,  0,  0,  0,  0,  1,  1,  1,  0, -1,  0,  0,  0,  1,  1, -1],
        ]                                
# 고도에 따른 값
# 0-10 : 1, 10-20 : 2, 20-30 : -1, 30-40 : -2
        

# class Agent :
#     def __init__(self) :
#         self.x = len(field) // 2
#         self.y = len(field) // 2



# import numpy as np  # numpy 라이브러리를 np라는 이름으로 가져옴

# def transition_prob(s, a):
#     """
#     현재 상태 s에서 행동 a를 했을 때의 전이 확률을 반환하는 함수입니다.
#     여기서는 단순화를 위해 결정론적인 전이만 다루고 있습니다.
#     """
#     # 위, 아래, 왼쪽, 오른쪽 이동에 따른 새로운 상태 결정
#     if a == 0: # 상으로 이동
#         return s-4 if s-4 >= 0 else s
#     elif a == 1: # 하로 이동
#         return s+4 if s+4 <= 15 else s
#     elif a == 2: # 좌로 이동
#         return s-1 if s % 4 != 0 else s
#     elif a == 3: # 우로 이동
#         return s+1 if s % 4 != 3 else s

# def value_iteration(states, actions, r, gamma=0.99, threshold=0.001):
#     V = np.zeros(len(states))  # 각 상태에 대한 가치를 저장할 배열 초기화
#     policy = np.zeros(len(states), dtype=int)  # 각 상태에서 선택할 최적 행동을 저장할 배열 초기화

#     while True:
#         delta = 0  # 가치 함수가 얼마나 변경되었는지 추적
#         for s in states:  # 모든 상태에 대해 반복
#             V_temp = V[s]  # 현재 상태의 가치를 임시 변수에 저장
#             # 가능한 모든 행동에 대해 새로운 가치 계산하고 최대값을 현재 상태의 가치로 설정
#             V[s] = max([r[transition_prob(s, a)] + gamma * V[transition_prob(s, a)] for a in actions])
#             delta = max(delta, abs(V_temp - V[s]))  # 가장 큰 가치 변화를 delta에 저장
#         if delta < threshold:  # 변화량이 threshold보다 작으면 반복을 중단
#             break

#     for s in states:  # 모든 상태에 대해 반복
#         # 최적의 행동을 선택하는 정책 계산
#         policy[s] = np.argmax([r[transition_prob(s, a)] + gamma * V[transition_prob(s, a)] for a in actions])
    
#     return policy, V  # 최적 정책과 가치 함수 반환

# states = list(range(16))  # 상태 공간 정의 (0부터 15까지 16개의 상태)
# actions = [0, 1, 2, 3]  # 가능한 행동 집합 정의 (상, 하, 좌, 우)
# r = [-0.01 for _ in states]  # 각 상태에 대한 보상 배열 초기화 (기본적으로 모두 -0.01)
# r[15] = 1  # 목표 상태(상태 15)에 대한 보상을 1로 설정

# optimal_policy, optimal_values = value_iteration(states, actions, r)  # 가치 반복 함수를 호출하여 최적 정책과 가치 계산
# print("최적 정책:", optimal_policy)  # 최적 정책 출력
# print("최적 가치:", optimal_values)  # 최적 가치 출력
import numpy as np
states = np.zeros((len(HIDE), len(HIDE[0])))
def every() :
        appendlist = []
        for i in range(len(HIDE)) :
                for k in range(len(HIDE[i])) :
                        states[i][k] = (HIDE[i][k] + HP[i][k] + SLOPE[i][k])
                
actions = [0, 1, 2, 3]
# r = [-0.001 for _ in states]
# r[15] = 1

# def p(s, a) :
    
#         if a == 0: # 상으로 이동
#                 return s-4 if s-4 >= 0 else s
#         elif a == 1: # 하로 이동
#                 return s+4 if s+4 <= 15 else s
#         elif a == 2: # 좌로 이동
#                 return s-1 if s % 4 != 0 else s
#         elif a == 3: # 우로 이동
#                 return s+1 if s % 4 != 3 else s

# def p(s, a) :
      
#       if a == 0 :
#             return s - 4 if s - 4 >= 0 else s
#       elif a == 1 :
#             return s + 4 if s + 4 < len(states) else s
#       elif a == 2 :
#             return s - 1 if s % 4 != 0 else s
#       elif a == 3 :
#             return s + 1 if s % 4 != 3 else s


# def value_re(states, actions, r, gamma = 0.9, threshold = 0.001) :
#         v = np.zeros(len(states))
#         policy = np.zeros(len(states), dtype=int)
#         while True :
#                 delta = 0

#                 for s in states :
#                         temp_v = v[s]

#                         v[s] = max([r[p(s, a)] + gamma * v[p(s, a)] for a in actions])
#                         delta = max(delta, abs(temp_v - v[s]))
#                 if delta < threshold :
#                         break
        
#         for s in states :
#                policy[s] = np.argmax([r[p(s, a)] + gamma * v[p(s, a)] for a in actions])

#         return policy, v

# op_p, op_v = value_re(states, actions, r)
# print(op_p)

# print(op_v)

# for i in range(len(op_v)) :
#        if i % 4 == 0 :
#               print()
#        print(round(op_v[i], 1), end = "  ")

# for i in range(len(op_v)) :
#        if i % 4 == 0 :
#               print()
#        print(round(op_p[i], 1), end = "  ")
every()
for i in states :
        print(i)